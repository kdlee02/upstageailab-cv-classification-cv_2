{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì›ë³¸ ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
      "ë¶„ë¦¬ ê²°ê³¼ -> í•™ìŠµìš©: 785ê°œ, ê²€ì¦ìš©: 785ê°œ\n",
      "\n",
      "ğŸš€ í•™ìŠµ ë°ì´í„° ì¦ê°• ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„° ì¦ê°•:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„° ì¦ê°•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [3:24:02<00:00, 510.11s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: 76800ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ './data/train_augmented_v2/images'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸš€ ê²€ì¦ ë°ì´í„° ì¦ê°• ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ë°ì´í„° ì¦ê°•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [3:24:09<00:00, 510.40s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: 76800ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ './data/val_augmented_v2/images'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from timm.data.mixup import Mixup\n",
    "import albumentations as A\n",
    "from augraphy import AugraphyPipeline, VoronoiTessellation\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. ê¸°ë³¸ ì„¤ì •\n",
    "# --------------------------------------------------------------------------\n",
    "# ì›ë³¸ ë°ì´í„° ê²½ë¡œ\n",
    "ORIGINAL_CSV_PATH = './data/train.csv'\n",
    "ORIGINAL_IMG_DIR = './data/train/'\n",
    "\n",
    "# ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ë  ë°ì´í„° í´ë” ê²½ë¡œ\n",
    "FINAL_TRAIN_DIR = './data/train_augmented_v2/'\n",
    "FINAL_VAL_DIR = './data/val_augmented_v2/'\n",
    "\n",
    "# ì¦ê°• ë° ë¶„ë¦¬ ê´€ë ¨ íŒŒë¼ë¯¸í„°\n",
    "VAL_SIZE = 0.5\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 17\n",
    "AUGMENTATION_COUNT = 100 # ë°ì´í„°ì…‹ ë‹¹ ì ìš©í•  ì¦ê°• íšŸìˆ˜\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë° ì´ˆê¸° ë³€í™˜ (ì¬ì‚¬ìš©)\n",
    "# --------------------------------------------------------------------------\n",
    "class ResizeWithPadding:\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆí•˜ê³ , ë‚¨ëŠ” ê³µê°„ì„ íŒ¨ë”©ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, fill=(0, 0, 0)):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        scale = min(self.size[0] / w, self.size[1] / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        resized = TF.resize(image, (new_h, new_w))\n",
    "        pad_left = (self.size[0] - new_w) // 2\n",
    "        pad_top = (self.size[1] - new_h) // 2\n",
    "        pad_right = self.size[0] - new_w - pad_left\n",
    "        pad_bottom = self.size[1] - new_h - pad_top\n",
    "        return TF.pad(resized, (pad_left, pad_top, pad_right, pad_bottom), fill=self.fill)\n",
    "\n",
    "initial_transform = T.Compose([\n",
    "ResizeWithPadding((591, 443), fill=(255, 255, 255)),\n",
    "T.ToTensor(),\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['ID']\n",
    "        label = self.df.iloc[idx]['target']\n",
    "        image_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ (ì¦ê°• ë° ì €ì¥ ë¡œì§ í†µí•©)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def augment_and_save_dataset(df, base_dir, csv_name, description):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì— ì¦ê°•ì„ ì ìš©í•˜ê³  ì§€ì •ëœ ê²½ë¡œì— ì´ë¯¸ì§€ì™€ CSVë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸš€ {description} ì‹œì‘...\")\n",
    "\n",
    "    # --- ê²½ë¡œ ì„¤ì • ë° í´ë” ìƒì„± ---\n",
    "    output_img_dir = os.path.join(base_dir, 'images')\n",
    "    output_csv_path = os.path.join(base_dir, csv_name)\n",
    "    os.makedirs(output_img_dir, exist_ok=True)\n",
    "\n",
    "    # --- ì¦ê°• íŒŒì´í”„ë¼ì¸ ì •ì˜ ---\n",
    "    mixup_fn = Mixup(mixup_alpha=0.5, cutmix_alpha=0.0, prob=1.0, label_smoothing=0.1, num_classes=NUM_CLASSES)\n",
    "    augraphy_pipeline = AugraphyPipeline([\n",
    "    VoronoiTessellation(num_cells_range=(150, 250), p=0.5)\n",
    "    ])\n",
    "    albumentations_transform = A.Compose([\n",
    "    # A.GaussianBlur(blur_limit=(5, 7), p=1.0),\n",
    "    A.Rotate(limit=360, p=0.9, border_mode=0, value=(255, 255, 255)),\n",
    "    ])\n",
    "\n",
    "    # --- ë°ì´í„° ë¡œë”© ë° ì¦ê°• ---\n",
    "    dataset = CustomImageDataset(df=df, image_dir=ORIGINAL_IMG_DIR, transform=initial_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE,drop_last=True)\n",
    "\n",
    "    new_metadata = []\n",
    "    for images, labels in tqdm(dataloader, desc=description):\n",
    "        for _ in range(AUGMENTATION_COUNT):\n",
    "            if images.size(0) % 2 != 0:\n",
    "                images = torch.cat([images, images[-1].unsqueeze(0)], dim=0)\n",
    "                labels = torch.cat([labels, labels[-1].unsqueeze(0)], dim=0)\n",
    "            mixed_images, mixed_labels = mixup_fn(images, labels)\n",
    "            for i in range(mixed_images.size(0)):\n",
    "                img_tensor, label_array = mixed_images[i], mixed_labels[i].cpu().numpy()\n",
    "                img_np = np.array(TF.to_pil_image(img_tensor))\n",
    "\n",
    "                # Augraphy -> Albumentations ì ìš©\n",
    "                augmented_np = albumentations_transform(image=augraphy_pipeline(image=img_np))[\"image\"]\n",
    "\n",
    "                final_image_pil = Image.fromarray(augmented_np)\n",
    "                filename = f\"{uuid.uuid4()}.png\"\n",
    "                final_image_pil.save(os.path.join(output_img_dir, filename))\n",
    "\n",
    "                new_metadata.append({'ID': filename, 'target': str(list(label_array))})\n",
    "\n",
    "    df_new = pd.DataFrame(new_metadata)\n",
    "    df_new.to_csv(output_csv_path, index=False)\n",
    "    print(f\"âœ… ì™„ë£Œ: {len(df_new)}ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ '{output_img_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- 1. ë°ì´í„° ë¶„ë¦¬ ---\n",
    "print(\"ğŸ“Š ì›ë³¸ ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "df_original = pd.read_csv(ORIGINAL_CSV_PATH)\n",
    "\n",
    "df_train, df_val = train_test_split(\n",
    "df_original,\n",
    "test_size=VAL_SIZE,\n",
    "shuffle=True,\n",
    "random_state=RANDOM_STATE,\n",
    "stratify=df_original['target']\n",
    ")\n",
    "print(f\"ë¶„ë¦¬ ê²°ê³¼ -> í•™ìŠµìš©: {len(df_train)}ê°œ, ê²€ì¦ìš©: {len(df_val)}ê°œ\")\n",
    "\n",
    "# --- 2. í•™ìŠµ ë°ì´í„° ì¦ê°• ë° ì €ì¥ ---\n",
    "augment_and_save_dataset(\n",
    "    df=df_train,\n",
    "    base_dir=FINAL_TRAIN_DIR,\n",
    "    csv_name='train_augmented_v2.csv',\n",
    "    description=\"í•™ìŠµ ë°ì´í„° ì¦ê°•\"\n",
    ")\n",
    "\n",
    "# --- 3. ê²€ì¦ ë°ì´í„° ì¦ê°• ë° ì €ì¥ ---\n",
    "augment_and_save_dataset(\n",
    "    df=df_val,\n",
    "    base_dir=FINAL_VAL_DIR,\n",
    "    csv_name='val_augmented_v2.csv',\n",
    "    description=\"ê²€ì¦ ë°ì´í„° ì¦ê°•\"\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ì›ë³¸ ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
      "ë¶„ë¦¬ ê²°ê³¼ -> í•™ìŠµìš©: 785ê°œ, ê²€ì¦ìš©: 785ê°œ\n",
      "\n",
      "ğŸš€ í•™ìŠµ ë°ì´í„° ì¦ê°• ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„° ì¦ê°•:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„° ì¦ê°•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:15<00:00,  5.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: 768ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ './data/train_augmented_v2/images'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸš€ ê²€ì¦ ë°ì´í„° ì¦ê°• ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ë°ì´í„° ì¦ê°•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [02:12<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì™„ë£Œ: 768ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ './data/val_augmented_v2/images'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "from timm.data.mixup import Mixup\n",
    "import albumentations as A\n",
    "from augraphy import AugraphyPipeline, VoronoiTessellation\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. ê¸°ë³¸ ì„¤ì •\n",
    "# --------------------------------------------------------------------------\n",
    "# ì›ë³¸ ë°ì´í„° ê²½ë¡œ\n",
    "ORIGINAL_CSV_PATH = './data/train.csv'\n",
    "ORIGINAL_IMG_DIR = './data/train/'\n",
    "\n",
    "# ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ë  ë°ì´í„° í´ë” ê²½ë¡œ\n",
    "FINAL_TRAIN_DIR = './data/train_augmented_v2/'\n",
    "FINAL_VAL_DIR = './data/val_augmented_v2/'\n",
    "\n",
    "# ì¦ê°• ë° ë¶„ë¦¬ ê´€ë ¨ íŒŒë¼ë¯¸í„°\n",
    "VAL_SIZE = 0.5\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 17\n",
    "AUGMENTATION_COUNT = 100 # ë°ì´í„°ì…‹ ë‹¹ ì ìš©í•  ì¦ê°• íšŸìˆ˜\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë° ì´ˆê¸° ë³€í™˜ (ì¬ì‚¬ìš©)\n",
    "# --------------------------------------------------------------------------\n",
    "class ResizeWithPadding:\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆí•˜ê³ , ë‚¨ëŠ” ê³µê°„ì„ íŒ¨ë”©ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, fill=(0, 0, 0)):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        scale = min(self.size[0] / w, self.size[1] / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        resized = TF.resize(image, (new_h, new_w))\n",
    "        pad_left = (self.size[0] - new_w) // 2\n",
    "        pad_top = (self.size[1] - new_h) // 2\n",
    "        pad_right = self.size[0] - new_w - pad_left\n",
    "        pad_bottom = self.size[1] - new_h - pad_top\n",
    "        return TF.pad(resized, (pad_left, pad_top, pad_right, pad_bottom), fill=self.fill)\n",
    "\n",
    "initial_transform = T.Compose([\n",
    "    ResizeWithPadding((591, 443), fill=(255, 255, 255)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['ID']\n",
    "        label = self.df.iloc[idx]['target']\n",
    "        image_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ (ì¦ê°• ë° ì €ì¥ ë¡œì§ í†µí•©)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def augment_and_save_dataset(df, base_dir, csv_name, description):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì— ì¦ê°•ì„ ì ìš©í•˜ê³  ì§€ì •ëœ ê²½ë¡œì— ì´ë¯¸ì§€ì™€ CSVë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸš€ {description} ì‹œì‘...\")\n",
    "\n",
    "    # --- ê²½ë¡œ ì„¤ì • ë° í´ë” ìƒì„± ---\n",
    "    output_img_dir = os.path.join(base_dir, 'images')\n",
    "    output_csv_path = os.path.join(base_dir, csv_name)\n",
    "    os.makedirs(output_img_dir, exist_ok=True)\n",
    "\n",
    "    # --- ì¦ê°• íŒŒì´í”„ë¼ì¸ ì •ì˜ ---\n",
    "    mixup_fn = Mixup(mixup_alpha=0.5, cutmix_alpha=0.0, prob=1.0, label_smoothing=0.1, num_classes=NUM_CLASSES)\n",
    "    augraphy_pipeline = AugraphyPipeline([\n",
    "        VoronoiTessellation(num_cells_range=(150, 250), p=0.5)\n",
    "    ])\n",
    "    albumentations_transform = A.Compose([\n",
    "        # A.GaussianBlur(blur_limit=(5, 7), p=1.0),\n",
    "        A.Rotate(limit=360, p=0.9, border_mode=0, value=(255, 255, 255)),\n",
    "    ])\n",
    "\n",
    "    # --- ë°ì´í„° ë¡œë”© ë° ì¦ê°• ---\n",
    "    dataset = CustomImageDataset(df=df, image_dir=ORIGINAL_IMG_DIR, transform=initial_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZEArithmeticError)\n",
    "\n",
    "    new_metadata = []\n",
    "    for images, labels in tqdm(dataloader, desc=description):\n",
    "        for _ in range(AUGMENTATION_COUNT):\n",
    "            if images.size(0) % 2 != 0:\n",
    "                # ë§ˆì§€ë§‰ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ë³µì œí•˜ì—¬ ì´ì–´ ë¶™ì„\n",
    "                images = torch.cat([images, images[-1].unsqueeze(0)], dim=0)\n",
    "                labels = torch.cat([labels, labels[-1].unsqueeze(0)], dim=0)\n",
    "            mixed_images, mixed_labels = mixup_fn(images, labels)\n",
    "            for i in range(mixed_images.size(0)):\n",
    "                img_tensor, label_array = mixed_images[i], mixed_labels[i].cpu().numpy()\n",
    "                img_np = np.array(TF.to_pil_image(img_tensor))\n",
    "\n",
    "                # Augraphy -> Albumentations ì ìš©\n",
    "                augmented_np = albumentations_transform(image=augraphy_pipeline(image=img_np))[\"image\"]\n",
    "\n",
    "                final_image_pil = Image.fromarray(augmented_np)\n",
    "                filename = f\"{uuid.uuid4()}.png\"\n",
    "                final_image_pil.save(os.path.join(output_img_dir, filename))\n",
    "\n",
    "                new_metadata.append({'ID': filename, 'target': str(list(label_array))})\n",
    "\n",
    "    df_new = pd.DataFrame(new_metadata)\n",
    "    df_new.to_csv(output_csv_path, index=False)\n",
    "    print(f\"âœ… ì™„ë£Œ: {len(df_new)}ê°œì˜ ì¦ê°• ì´ë¯¸ì§€ê°€ '{output_img_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
    "# --------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # --- 1. ë°ì´í„° ë¶„ë¦¬ ---\n",
    "    print(\"ğŸ“Š ì›ë³¸ ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "    df_original = pd.read_csv(ORIGINAL_CSV_PATH)\n",
    "\n",
    "    df_train, df_val = train_test_split(\n",
    "        df_original,\n",
    "        test_size=VAL_SIZE,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=df_original['target']\n",
    "    )\n",
    "    print(f\"ë¶„ë¦¬ ê²°ê³¼ -> í•™ìŠµìš©: {len(df_train)}ê°œ, ê²€ì¦ìš©: {len(df_val)}ê°œ\")\n",
    "\n",
    "    # --- 2. í•™ìŠµ ë°ì´í„° ì¦ê°• ë° ì €ì¥ ---\n",
    "    augment_and_save_dataset(\n",
    "        df=df_train,\n",
    "        base_dir=FINAL_TRAIN_DIR,\n",
    "        csv_name='train_augmented_v2.csv',\n",
    "        description=\"í•™ìŠµ ë°ì´í„° ì¦ê°•\"\n",
    "    )\n",
    "\n",
    "    # --- 3. ê²€ì¦ ë°ì´í„° ì¦ê°• ë° ì €ì¥ ---\n",
    "    augment_and_save_dataset(\n",
    "        df=df_val,\n",
    "        base_dir=FINAL_VAL_DIR,\n",
    "        csv_name='val_augmented_v2.csv',\n",
    "        description=\"ê²€ì¦ ë°ì´í„° ì¦ê°•\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.read_csv('./data/train_augmented_v2/train_augmented_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c\n",
       "16    50\n",
       "10    50\n",
       "8     50\n",
       "11    50\n",
       "7     49\n",
       "15    49\n",
       "2     49\n",
       "3     49\n",
       "9     49\n",
       "4     49\n",
       "12    48\n",
       "6     47\n",
       "5     47\n",
       "0     47\n",
       "13    37\n",
       "14    25\n",
       "1     23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "a = pd.read_csv('data/train_augmented_v2/train_augmented_v2.csv')\n",
    "a['target'] = a['target'].apply(lambda x : ast.literal_eval(x))\n",
    "a['c'] = a['target'].apply(lambda x : x.index(max(x)))\n",
    "a['c'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
